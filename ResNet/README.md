ResNet网络

论文地址:https://arxiv.org/abs/1512.03385

## Abstract
深度神经网络变得更加难以训练，我们提出了一种残差网络网络模型以尝试去解决这一问题

我们明确的将网络层重新规划为学习关于每层输入的残差函数---在数理统计中，残差是指实际观察值与估计值（拟合值）之间的差---而不是学习无参照函数（翻译的不是太明白）

(We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions)

我们提供了以实验为依据的证据，用以表明残差网络是更容易去最优化，并且可以从相当大的深度中获取精确度（一般的网络是这样的：随着网络深度的增加，训练精确度达到了饱和，然后再急剧衰退）。此外，深度残差网络相比较于1/8深VGG网络还更简单（网络越复杂，越容易过拟合）。

## 1. Introduction
这里并没有完全按照论文翻译,而是提取了重点。

在图片分类、识别定位领域，深度神经网络现在已经是一个非常重要的存在， 但是，这里提出了一个问题.是否学习一个更优的网络，是否跟堆积木一样简单。因为，网络越深，越难学习--梯度消失/爆炸然而，这个问题吧，在十层使用SGD优化器的网络上，可以通过初始标准化、中间标准化解决，

但是吧，当一个更深的网络开始收敛的时候，有个退化的问题就暴露出来了:随着网络深度的增加，训练精度达到了饱和，随后快速下降，精度下降并不是过度拟合导致的，并且更多层数会让模型导致更多的训练错误,如下图所示:

![img](https://user-images.githubusercontent.com/28073374/135264937-b43d4823-df9c-4cc2-9a26-544e2fd7d124.png)

训练精度的下降揭示了一个现象，不是所有的网络都相似的很容易优化。让我们考虑一个浅一点的架构，并且这个架构层数（Let us consider a
shallower architecture and its deeper counterpart that adds
more layers onto it.）考虑一个浅的网络结构，并且并且这个网络与添加更多网络层次的网络的网络深度一致。按我的理解就是浅网络新架构来替代深网络。从而解决网络过深带来的问题

一个已存的解决方案就是： 增加的层是身份映射，并且其他的层是从学习到的较浅的模型中复制出来的。这个方案可能并不太合适
(There exists a solution by construction
to the deeper model: the added layers are identity mapping,
and the other layers are copied from the learned shallower
model.)

在这个论文中，作者提出了一种解决方案 deep residual learning framework 残差网络学习模型。我们明确的让这些网络层去拟合残差映射，而并没有期望每一堆叠层去直接拟合一个期望的潜在映射（拟合函数）。正式地，我们将期望的潜在映射指代为H(x)，让堆积的非线性层匹配其他的映射F(x)：H(x) - x.
而最初的映射被重新投射为F(x) + x. **我们假设：相较于最优化最初的无参照映射（残差函数以输入x作为参照），最优化残差映射是更容易的**。极限情况下，如果残差映射是最优化的，相较于通过一堆非线性层去拟合恒等映射，将残差逼近为0是更容易的（我们去最优化目标函数即最小化损失函数，通过链式法则，相当于将残差逼近为0）。

F(x) + x 公式可以通过具有“shortcut connections（捷径连接）”的前向传递神经网络得以实现，如下图：

![img_1](https://user-images.githubusercontent.com/28073374/135264953-91bf1214-3a76-458c-8841-a9e204a8c81d.png)

shortcut connections 是指向跳过一或者多层。在我们的案例中，shortcut connections仅仅执行identity mapping，并且其输出与堆叠的网络层输出相加。恒等的捷径连接既没有增加额外的参数又没有增加计算复杂度。整个网络任然可以通过具备反向传递的SGD算法进行端到端的训练，并且可以很容易的使用公共类库（caffe/caffe2）进行实现，而不需要修改求解程序。

我们在ImageNet上呈现了综合性的试验，用以表明衰退问题及评估我们的方法。我们得出了：1) 我们非常深的残差网络很容易最优化，但是当网络的深度加深时，与残差网络相对应的朴素网络（without shortcut connections）则表现了更高的训练错误；2) 我们的深度残差网络可以很容易的从急剧增加的深度中获得精确度，产生的结果大体上更好于先前的网络。如下图3：

## 2. Related Work

**Residual Representations**

TODO

**Shortcut Connections**

TODO

## 3. Deep Residual Learning
### 3.1. Residual Learning
让我们考虑H(x)作为特征映射，由基层堆叠而成(不一定是整个网络)，x表示第一层的输入，如果假设多个非线性层能够逐渐逼近复杂函数，那么相同的也假设他能够逐渐逼近残差函数，即H(x)-x(假设输入以及输出是相同的维度)。
所以并不希望堆叠网络能逼近H(x)，而是显示的让这些层近似于残差网络F(x) := H(x)-x.原始函数最终成为了F(x)+x。尽管这这两种形式都能够逐渐的逼近所需函数(假设)，但是学习的程度可能不同

重新这么表述的动机是基于梯度退化的反常现象提出的。正如我们在Introduction中所讨论的，如果添加的层可以被构造为特征映射，那么一个深度的网络的训练误差不应该大于较浅的误差。退化问题表明，通过多个非线性层的解决方案在近似特征映射上可能有些困难。
随着残差学习的重新表述，如果特征映射是最优解的，则解决方案可以简单地将多个非线性层的权重推向零，以接近单位映射
    
在真实的情况中，特征映射不太可能是最优解。但是我们的重新的表述的残差学习可能有助于去预先设定这个问题。如果一个最优函数更接近特征映射而不是空映射，那么发现一个查找与恒等式相关的扰动
映射就更加简单，而不是去学习一个新的函数。我们展示，通过实验(如下图)，学习到的残差函数 一般来说，他们的反应很小，这表明了，特征映射提供了合理的预处理。

![img_4](https://user-images.githubusercontent.com/28073374/135264976-50658a90-9d76-4b3b-998f-b61fa3438c49.png)

### 3.2. Identity Mapping by Shortcuts
![img_5](https://user-images.githubusercontent.com/28073374/135265028-a290b31b-8692-4758-9a0c-9e976249f3a7.png)

我们每隔几个堆叠网络使用残差学习，构建块如上图图所示。形式上，在这篇论文中，我们将构建块定义为:

y = F(x,{w}) +x

x以及y是向量层的输入与输入。函数F(x,{w})展示需要被被学习的残差映射。对于这个上图的结构，该结构一共有两层，F = W2 σ(W1x) ，σ 表示ReLU 以及为了简化，省略了偏差。F+x就是短连接以及元素加成的相加( The operation F + x is performed by a shortcut connection and element-wise addition.)

短连接中既不需要引入额外的参数，也不引入复杂的计算，这不仅仅在实践中很有吸引力，同样的在我们的简单的网络和残差网络比较中也很重要。我们可以公平的比较同时具有相同数量、的横渡以及计算量和计算成本(除可忽略的元素添加外)

在等式中，x与F的尺寸必须相等，如果不相等，则可以公国改变通道的方式去匹配。

y = F(x, {Wi}) + Wsx.

我们能使用平方矩阵Ws。但我们会的 通过实验证明，特征映射是充分的 用于解决退化问题，并且是经济的， 因此，Ws仅在匹配维度时使用。

残差函数F的形式是灵活的。本文中的实验涉及一个具有两个或多个参数的函数F 三层（图5），而更多层是可能的。但如果 F只有一层，等式（1）类似于线性层： y=W1x+x，我们尚未观察到这方面的优势。 我们还注意到，尽管上述符号是关于 完全连接的层为简单起见，它们适用于卷积层。函数F（x，{Wi}）可以表示多个卷积层。逐通道在两个特征贴图上执行元素添加

### 3.3. Network Architectures

我们已经测试了大量的普通/残差网络，并且已经观察到了大象一致的现象。为了提供讨论的实例，接下来我们为ImageNet描述了两种模型。如图

![img_6](https://user-images.githubusercontent.com/28073374/135265008-c8f2bf4c-0fcb-45b6-a470-b6064432b885.png)

**Plain Network.**
普通的网络，上图中间的网络，受VGG网络的启发。卷积层大多有3x3过滤器随后跟着两个简单的规则:1,对于相同的输出的特征图大小，图层具有相同数量的过滤器。2，如果特征图大小减半，则过滤器的数量将加倍，以保持每层的时间复杂度。我们通过以下方式直接执行下采样： 步幅为2的卷积层。网络以全局平均池层和1000路结束与softmax完全连接的层。

值得注意的是，此网络相比于VGG有更少的过滤器以及更少的计算量。34层有3600000的FLOPs，仅仅只有VGG-19的百分18.

**Residual Network.**
残差网络，上图右边的网络。基于上面的普通网络，我们插入了短连接。这将次网络转变成为了相对的残差版本。当输入与输出同维度的情况下，特征的短连接能被直接的使用。当尺寸增加时，我们考虑了两种选项
(A)短连接继续作用与特征映射上，使用0填充，来增加尺寸。(B)用匹配尺寸（通过1×1卷积完成）。两者皆有 选项，当快捷方式跨越两个图形的要素贴图时 尺寸，它们以2的步幅执行。

### 3.4. Implementation
具体的实施，

我们对ImageNet的实现遵循这种做法 在[21,40]中。在[256，480]中对图像的较短侧进行随机采样，以调整图像的大小，以进行缩放[40]。 224×224作物是从图像或其图像中随机取样的 水平翻转，减去每像素平均值[21]。这个 使用了[21]中的标准颜色增强。我们采用批量生产 归一化（BN）[16]在每次卷积和 激活前，在[16]之后。我们初始化权重 如[12]所述，从零开始训练所有平网/残余网。我们 使用最小批量为256的SGD。学习率 从0.1开始，当误差稳定时除以10， 模型的训练次数可达60×104 迭代。我们 使用0.0001的重量衰减和0.9的动量。我们 按照[16]中的做法，不要使用辍学[13]。 在测试中，我们采用标准进行比较研究 10作物测试[21]。为了获得最佳结果，我们采用了[40,12]中所述的完全进化形式，并对分数进行平均 在多个比例下（图像的大小调整为 边在{22，42，56，38，48，06，40}中。

## 4. Experiments
### 4.1. ImageNet Classification
### 4.2. CIFAR-10 and Analysis
### 4.3. Object Detection on PASCAL and MS COCO

## 5. 各种数据的结果
略
