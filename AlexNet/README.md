# AlexNetè®ºæ–‡çš„è§£è¯»

**è®ºæ–‡åœ°å€ TODO**

## ä¸ªäººæ€»ç»“
è¯¥æ–‡ç« æå‡ºäº†å…³äºç½‘ç»œæ”¹å–„æ€§èƒ½ï¼Œå‡å°‘è®­ç»ƒæ—¶é—´ä¸€äº›æŠ€å·§(Reluã€å¤šå—GPUè®­ç»ƒã€LRNã€é‡å æ± åŒ–å±‚ã€dropOutç­‰)ï¼Œåœ¨å…¶paperä¸­çš„ç»“æœä¸­å¯ä»¥çœ‹åˆ°ï¼Œç¡®å®å¯¹äºè®­ç»ƒè¿‡ç¨‹ä»¥åŠæ€§èƒ½æœ‰ä¸€å®šçš„å¸®åŠ©

## Abstract
åœ¨LSVRC_2010çš„æµ‹è¯•é›†ä¸­ï¼Œå–å¾—äº†Top-1çš„é”™è¯¯ç‡æ˜¯37.5%ï¼ŒTop-5çš„é”™è¯¯ç‡æ˜¯17%(è¿™é‡Œè§£é‡Šä¸‹ä»€ä¹ˆæ˜¯Top-1:å°±æ˜¯æŒ‡ç¬¬ä¸€ç§ç±»åˆ«ä¸Šçš„ç²¾ç¡®åº¦æˆ–è€…é”™è¯¯ç‡ï¼Œè¿™é‡ŒæŒ‡çš„é”™è¯¯ç‡ï¼ŒTop-5æŒ‡çš„æ˜¯å‰äº”ç§ç±»åˆ«çš„æ€»é”™è¯¯ç‡)ã€‚è¯¥ç½‘ç»œæœ‰60M çš„å‚æ•°é‡650000çš„ç¥ç»å…ƒæ•°é‡.
5ä¸ªå·ç§¯å±‚ï¼ŒæŸäº›å·ç§¯å±‚åä¹Ÿè·Ÿç€æ± åŒ–å±‚ï¼Œéšåè¿˜æœ‰ä¸‰å±‚å…¨è¿æ¥ï¼Œæœ€åsoftmaxåˆ é™¤åˆ†ç±»1000.åŒæ—¶ä¸ºäº†æ›´å¿«çš„è®­ç»ƒï¼Œä½¿ç”¨äº†éé¥±å’Œç¥ç»å…ƒï¼ˆnon-saturatingï¼‰ï¼Œä»¥åŠæœ‰æ•ˆçš„GPUè®­ç»ƒæ–¹å¼ã€‚
åŒæ—¶ä¸ºäº†å‡å°‘è¿‡æ‹Ÿåˆï¼Œä½¿ç”¨äº†dropOut.æœ€ç»ˆåœ¨ILSVRC_2012ä¸­ï¼Œå–å¾—äº†top-5 é”™è¯¯ç‡15.3%.

psï¼š è¿™é‡Œä¸çŸ¥é“æ˜¯ä¸æ˜¯é¦–æ¬¡ä½¿ç”¨äº†DropOutï¼Œä½†æ˜¯åœ¨ä¹‹å‰çš„çœ‹çš„è®ºæ–‡å½“ä¸­ï¼Œå¹¶æ²¡æœ‰ä¸“é—¨ä½“ç§¯DropOutçš„ä½¿ç”¨ã€‚

## 1 Introduction
**ç•¥è¿‡å‰é¢ï¼Œçœ‹åé¢æ®µè½é‡ç‚¹:**

æœ€åï¼Œç½‘ç»œçš„å¤§å°ä¸»è¦å—åˆ°å½“å‰GPUä¸Šå¯ç”¨å†…å­˜é‡çš„é™åˆ¶ æˆ‘ä»¬æ„¿æ„å®¹å¿çš„è®­ç»ƒæ—¶é—´ã€‚æˆ‘ä»¬çš„ç½‘ç»œéœ€è¦5åˆ†é’Ÿ åœ¨ä¸¤ä¸ªGTX 580 3GB GPUä¸Šè®­ç»ƒ6å¤©ã€‚æˆ‘ä»¬æ‰€æœ‰çš„å®éªŒéƒ½è¡¨æ˜æˆ‘ä»¬çš„ç»“æœ åªéœ€ç­‰å¾…æ›´å¿«çš„GPUå’Œæ›´å¤§çš„æ•°æ®é›†å¯ç”¨ï¼Œå°±å¯ä»¥æé«˜æ€§èƒ½ã€‚

(In the end, the networkâ€™s size is limited mainly by the amount of memory available on current GPUs
and by the amount of training time that we are willing to tolerate. Our network takes between five
and six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results
can be improved simply by waiting for faster GPUs and bigger datasets to become available.)

å…¸å‹çš„å¤šå¡è®­ç»ƒï¼Œä½†æ˜¯GTX580æ˜¾å¡ä¸å¤ªè¡Œäº†ï¼Œ3GBä¹Ÿæœ‰ç‚¹å°ã€‚æœ¬åœ°æ˜¯1660Ti 6GBè™½ç„¶åŒæ ·æ¸£ï¼Œä½†æ˜¯å®Œå…¨èƒ½è¾¾åˆ°ä½œè€…è®­ç»ƒçš„æ•ˆæœ

## 2 The Dataset
**ç•¥**

## 3 The Architecture

![img_1](https://user-images.githubusercontent.com/28073374/134807475-6c87e656-8e39-4827-8ebf-2e432549fbb9.png)

**å…³äºæ­¤ç»“æ„çš„ä¸€äº›è§£é‡Šï¼š**

denseï¼šå³å…¨è¿æ¥å±‚

### 3.1 ReLU Nonlinearity

è¿™é‡Œè¦å¥½å¥½çœ‹çœ‹äº†ï¼Œè¿™é‡Œæå‡ºäº†Reluçš„ä½¿ç”¨ã€‚

ä¹‹å‰ç½‘ç»œä¸­å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°éƒ½æ˜¯tanhï¼ˆ tanh(x)ï¼‰æˆ–è€…sigmoidï¼ˆf(x) = (1 + e âˆ’x)âˆ’1ï¼‰

(The standard way to model a neuronâ€™s output f as
a function of its input x is with f(x) = tanh(x)
or f(x) = (1 + e âˆ’x)âˆ’1)

ä½†æ˜¯ä¸Šè¿°çš„æ¿€æ´»å‡½æ•°å³é¥±å’Œç¥ç»å…ƒï¼Œæ¯”éé¥±å’Œç¥ç»å…ƒè®­ç»ƒæ›´æ…¢(Relu)

(In terms of training time
with gradient descent, these saturating nonlinearities
are much slower than the non-saturating nonlinearity
f(x) = max(0, x).)

ä¸€äº›å…·ä½“çš„å·®å¼‚ï¼Œå¦‚å›¾ã€‚å…³äºä¸ºä½•ä½¿ç”¨Relu,ä¸»è¦ç›®çš„å°±æ˜¯å› ä¸ºæ›´å¿«å§

å¦å¤–å…³äºnon-saturating neuronsçš„è§£é‡Šï¼šåº”è¯¥å°±æ˜¯outputå€¼å¹¶ä¸åœ¨æŸä¸ªç‰¹å®šçš„åŒºé—´ï¼ŒæŸ¥äº†ä¸€äº›ç½‘ä¸Šçš„èµ„æ–™ï¼Œå¯èƒ½è¿™ç§è¯´æ³•æ¯”è¾ƒé è°±

![img_2](https://user-images.githubusercontent.com/28073374/134807478-22ca97c9-d6b9-45d3-94a4-06d066045b9b.png)

### 3.2 Training on Multiple GPUs
å…³äºå¤šGPUçš„è®­ç»ƒï¼Œç•¥

### 3.3 Local Response Normalization

å±€éƒ¨å“åº”æ ‡å‡†åŒ–ï¼ŒLRNå±‚ï¼Œä¸€ä¸ªNormalization æ“ä½œï¼Œè¿™ç¯‡å…·ä½“è®²çš„å•¥å‘¢ã€‚å…ˆçœ‹å…¬å¼ï¼Œåæ¥çœ‹ç½‘ä¸Šå¾ˆå¤šäººè¯´è¿™ä¸ªè¢«å„ç§bnæ›¿ä»£äº†ã€‚

![img_3](https://user-images.githubusercontent.com/28073374/134807482-9902f62b-b6cf-48b1-932b-a4f8c05842b8.png)


a^i x,y:è¡¨ç¤ºç¬¬ i ç‰‡ç‰¹å¾å›¾åœ¨ä½ç½®ï¼ˆx,yï¼‰è¿ç”¨æ¿€æ´»å‡½æ•° ReLU åçš„è¾“å‡º
N:æ˜¯ç‰¹å¾å›¾çš„æ€»æ•°
n:æ˜¯åŒä¸€ä½ç½®ä¸Šä¸´è¿‘çš„ feature map çš„æ•°ç›®
k,Î²,Î±,n:éƒ½æ˜¯å‚æ•°

### 3.4 Overlapping Pooling

é‡å æ± åŒ–å±‚ï¼Œ

æ± çš„å¤§å°ä¸ºz*z,å¦‚æœs(æ­¥é•¿)å°äºzçš„è¯ï¼Œåˆ™ä½œä¸ºé‡å æ± åŒ–å±‚

(If we set s < z, we obtain overlapping pooling) 

å†çœ‹çœ‹é‡å æ± åŒ–å±‚çš„ä½œç”¨ï¼Œs=2 ï¼Œz=3 æ¯”è¾ƒs=z=2.å‰è€…æœ€åçš„ç»“æœå¥½ï¼Œä¹Ÿä¸€å®šç¨‹åº¦ä¸Šå¯¹è§£å†³è¿‡åº¦æ‹Ÿåˆæœ‰å¸®åŠ©

(This is what we use throughout our
network, with s = 2 and z = 3. This scheme reduces the top-1 and top-5 error rates by 0.4% and
0.3%, respectively, as compared with the non-overlapping scheme s = 2, z = 2, which produces
output of equivalent dimensions. We generally observe during training that models with overlapping
pooling find it slightly more difficult to overfit.)

### 3.5 Overall Architecture

å›é¡¾å‰é¢çš„ç»“æ„å›¾ï¼š

8å±‚ç½‘ç»œï¼Œäº”å±‚å·ç§¯+ä¸‰å±‚å…¨è¿æ¥ï¼Œæœ€åä¸€å±‚çš„è¾“å‡ºé€šè¿‡softmaxç›´æ¥è¾“å‡º1000æ ‡ç­¾

(Now we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net
contains eight layers with weights; the first five are convolutional and the remaining three are fully connected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces
a distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression
objective, which is equivalent to maximizing the average across training cases of the log-probability
of the correct label under the prediction distribution.)

å†çœ‹è¿™ä¸€å¥è¯ï¼ŒReluæ¥åœ¨äº†æ¯ä¸ªå·ç§¯å’Œå…¨è¿æ¥ä¹‹å

(The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer)

ç”±äºæ˜¯åŒè·¯GPUè¿›è¡Œè®­ç»ƒï¼Œåœ¨æœ¬åœ°æ˜¯å•è·¯GPUï¼Œæ‰€ä»¥å®é™…è®­ç»ƒï¼Œå…³äºé€šé“æ•°å•¥çš„ éƒ½åº”è¯¥*2ä½¿ç”¨

## 4 Reducing Overfitting

æ¥ä¸‹æ¥è®²ä¸¤ä¸ªä¸»è¦çš„æ–¹å¼æ¥é˜²æ­¢è¿‡æ‹Ÿåˆ

### 4.1 Data Augmentation

**ç¬¬ä¸€ç§æ˜¯é€šè¿‡å›¾åƒçš„å¹³ç§»å’Œæ°´å¹³åå°„**

ï¼ˆThe first form of data augmentation consists of generating image translations and horizontal reflectionsï¼‰

**ç¬¬äºŒç§æ˜¯æ”¹å˜å›¾åƒä¸­RGBé€šé“çš„å¼ºåº¦è®­ç»ƒå›¾åƒ**

ï¼ˆThe second form of data augmentation consists of altering the intensities of the RGB channels in
training imagesï¼‰

**å–å¾—çš„æ•ˆæœ:**

This scheme reduces the top-1 error rate by over 1%.

## 4.2 Dropout
Drop out pè®¾ç½®ä¸º0.5

## 5 Details of learning

**æ¥ä¸‹æ¥æ—¶è®²å…³äºå­¦ä¹ çš„ç»†èŠ‚**

![img_4](https://user-images.githubusercontent.com/28073374/134807506-c61005c0-38a3-41bd-a19e-e9d5226a62bf.png)

ä¸€ä¸ªé€æ¸é€’å‡çš„å­¦ä¹ 

where i is the iteration index, v is the momentum variable,  is the learning rate, and D âˆ‚L âˆ‚w wi E Di is the average over the ith batch Di of the derivative of the objective with respect to w, evaluated at wi . 

## 6 Results
ç•¥

## 7 Discussion
ç•¥

