
### GoogleNet

```
    We propose a deep convolutional neural network architecture codenamed Inception,
                                                (体系结构)      (代号)    
which was responsible for setting the new state of the art for classification

and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014

(ILSVRC14). The main hallmark of this architecture is the improved utilization
                      （标志)                                 (adj)    (利用率)
of the computing resources inside the network. This was achieved by a carefully

crafted design that allows for increasing the depth and width of the network while

keeping the computational budget constant. To optimize quality, the architectural
             (计算预算不变)
decisions were based on the Hebbian principle and the intuition of multi-scale
                                     (原理) 
processing. One particular incarnation used in our submission for ILSVRC14 is
                    （具体化的化身）
called GoogLeNet, a 22 layers deep network, the quality of which is assessed in、

the context of classification and detection.

## 语法：
1：responsible for setting the... 不知为何使用ing
##

1 Introduction
In the last three years, mainly due to the advances of deep learning, more concretely convolutional
                                                                           (具体的说是)    
networks [10], the quality of image recognition and object detection has been progressing at a dramatic
                                                                                (有着巨大的发展)
pace. One encouraging news is that most of this progress is not just the result of more powerful
           (令人鼓舞)
hardware, larger datasets and bigger models, but mainly a consequence of new ideas, algorithms and
（硬件）
improved network architectures. No new data sources were used, for example, by the top entries in

the ILSVRC 2014 competition besides the classification dataset of the same competition for detection

purposes. Our GoogLeNet submission to ILSVRC 2014 actually uses 12 fewer parameters

than the winning architecture of Krizhevsky et al [9] from two years ago, while being significantly

more accurate. The biggest gains in object-detection have not come from the utilization of deep

networks alone or bigger models, but from the synergy of deep architectures and classical computer
                                                (协同作用)
vision, like the R-CNN algorithm by Girshick et al [6].


    In this paper, we will focus on an efficient deep neural network architecture for computer vision,
                                        (高效神经网络)
codenamed Inception, which derives its name from the Network in network paper by Lin et al [12]
                        (其名起源于)
in conjunction with the famous “we need to go deeper” internet meme [1]. In our case, the word

“deep” is used in two different meanings: first of all, in the sense that we introduce a new level of
                                                                             (引入)                                    
organization in the form of the “Inception module” and also in the more direct sense of increased

network depth. In general, one can view the Inception model as a logical culmination of [12]
                                                                   (逻辑顶点)               
while taking inspiration and guidance from the theoretical work by Arora et al [2]. The benefits
(同时)
of the architecture are experimentally verified on the ILSVRC 2014 classification and detection

challenges, on which it significantly outperforms the current state of the art.

2 Related Work

    Starting with LeNet-5 [10], convolutional neural networks (CNN) have typically had a standard
                                                                          (have had 过去一直有)                                         
structure – stacked convolutional layers (optionally followed by contrast normalization and maxpooling)
                (叠层卷积层)
are followed by one or more fully-connected layers. Variants of this basic design are
                                                      (这种基本设计的变种)                         
prevalent in the image classification literature and have yielded the best results to-date on MNIST,

CIFAR and most notably on the ImageNet classcifiation challenge [9, 21]. For larger datasets such

as Imagenet, the recent trend has been to increase the number of layers [12] and layer size [21, 14],

while using dropout [7] to address the problem of overfitting.

    Despite concerns that max-pooling layers result in loss of accurate spatial information, the same
convolutional network architecture as [9] has also been successfully employed for localization [9,
14], object detection [6, 14, 18, 5] and human pose estimation [19]. Inspired by a neuroscience
model of the primate visual cortex, Serre et al. [15] use a series of fixed Gabor filters of different sizes
in order to handle multiple scales, similarly to the Inception model. However, contrary to the fixed
2-layer deep model of [15], all filters in the Inception model are learned. Furthermore, Inception
layers are repeated many times, leading to a 22-layer deep model in the case of the GoogLeNet
model.

    Network-in-Network is an approach proposed by Lin et al. [12] in order to increase the representational
                
power of neural networks. When applied to convolutional layers, the method could be viewed
(代表性)
as additional 11 convolutional layers followed typically by the rectified linear activation [9]. This

enables it to be easily integrated in the current CNN pipelines. We use this approach heavily in our
                        集成到
architecture. However, in our setting, 1  1 convolutions have dual purpose: most critically, they

are used mainly as dimension reduction modules to remove computational bottlenecks, that would

otherwise limit the size of our networks. This allows for not just increasing the depth, but also the

width of our networks without significant performance penalty.

    The current leading approach for object detection is the Regions with Convolutional Neural Networks
    
(R-CNN) proposed by Girshick et al. [6]. R-CNN decomposes the overall detection problem
                                                 分解   
into two subproblems: to first utilize low-level cues such as color and superpixel consistency for
                                利用
potential object proposals in a category-agnostic fashion, and to then use CNN classifiers to identify

object categories at those locations. Such a two stage approach leverages the accuracy of bounding

box segmentation with low-level cues, as well as the highly powerful classification power of

state-of-the-art CNNs. We adopted a similar pipeline in our detection submissions, but have explored

enhancements in both stages, such as multi-box [5] prediction for higher object bounding

box recall, and ensemble approaches for better categorization of bounding box proposals


3 Motivation and High Level Considerations
两大缺点
    The most straightforward way of improving the performance of deep neural networks is by increasing

their size. This includes both increasing the depth – the number of levels – of the network and its

width: the number of units at each level. This is as an easy and safe way of training higher quality

models, especially given the availability of a large amount of labeled training data. However this

simple solution comes with two major drawbacks.
                                        （缺点）
    Bigger size typically means a larger number of parameters, which makes the enlarged network more
    
prone to overfitting, especially if the number of labeled examples in the training set is limited.

This can become a major bottleneck, since the creation of high quality training sets can be tricky
    
and expensive, especially if expert human raters are necessary to distinguish between fine-grained

visual categories like those in ImageNet (even in the 1000-class ILSVRC subset) as demonstrated

by Figure 1.

Another drawback of uniformly increased network size is the dramatically increased use of computational resources. 

For example, in a deep vision network, if two convolutional layers are chained,

any uniform increase in the number of their filters results in a quadratic increase of computation. If

the added capacity is used inefficiently (for example, if most weights end up to be close to zero),

then a lot of computation is wasted. Since in practice the computational budget is always finite, an
                                                                             计算预算总是有限的
efficient distribution of computing resources is preferred to an indiscriminate increase of size, even
                          高效的分配资源比随意的分配资源更加可取 
when the main objective is to increase the quality of results.

    The fundamental way of solving both issues would be by ultimately moving from fully connected

to sparsely connected architectures, even inside the convolutions. Besides mimicking biological

systems, this would also have the advantage of firmer theoretical underpinnings due to the groundbreaking work of Arora et al. [2]. 

Their main result states that if the probability distribution of

the data-set is representable by a large, very sparse deep neural network, then the optimal network

topology can be constructed layer by layer by analyzing the correlation statistics of the activations

of the last layer and clustering neurons with highly correlated outputs. 

Although the strict mathematical proof requires very strong conditions, the fact that this statement resonates with the well

known Hebbian principle – neurons that fire together, wire together – suggests that the underlying

idea is applicable even under less strict conditions, in practice.

    On the downside, todays computing infrastructures are very inefficient when it comes to numerical
    
calculation on non-uniform sparse data structures. Even if the number of arithmetic operations is

reduced by 100×, the overhead of lookups and cache misses is so dominant that switching to sparse

matrices would not pay off. The gap is widened even further by the use of steadily improving,

highly tuned, numerical libraries that allow for extremely fast dense matrix multiplication, exploiting the minute details of the underlying CPU or GPU hardware [16, 9]. Also, non-uniform sparse

models require more sophisticated engineering and computing infrastructure. Most current vision

oriented machine learning systems utilize sparsity in the spatial domain just by the virtue of employing convolutions. However, convolutions are implemented as collections of dense connections

to the patches in the earlier layer. ConvNets have traditionally used random and sparse connection

tables in the feature dimensions since [11] in order to break the symmetry and improve learning, the

trend changed back to full connections with [9] in order to better optimize parallel computing. The

uniformity of the structure and a large number of filters and greater batch size allow for utilizing

efficient dense computation.

    This raises the question whether there is any hope for a next, intermediate step: an architecture
                                                                      中间的                   建筑
that makes use of the extra sparsity, even at filter level, as suggested by the theory, but exploits our 3
        利用              额外的稀疏性
current hardware by utilizing computations on dense matrices. The vast literature on sparse matrix
                                                                        在稀疏矩阵计算上大量的文献 表明
computations (e.g. [3]) suggests that clustering sparse matrices into relatively dense submatrices
                                        聚集
tends to give state of the art practical performance for sparse matrix multiplication. It does not

seem far-fetched to think that similar methods would be utilized for the automated construction of
事实并非如此
non-uniform deep-learning architectures in the near future.

    The Inception architecture started out as a case study of the first author for assessing the hypothetical
                                                                                用于  评估          假设
output of a sophisticated network topology construction algorithm that tries to approximate a sparse
                复杂的      网络拓扑学      
structure implied by [2] for vision networks and covering the hypothesized outcome by dense, readily available components. 

Despite being a highly speculative undertaking, only after two iterations
                        投机          事业                          迭代
on the exact choice of topology, we could already see modest gains against the reference architecture based on [12]. 

After further tuning of learning rate, hyperparameters and improved training
                调整                          超参数
methodology, we established that the resulting Inception architecture was especially useful in the
                    最终确认了
context of localization and object detection as the base network for [6] and [5]. Interestingly, while

most of the original architectural choices have been questioned and tested thoroughly, they turned

out to be at least locally optimal.

    One must be cautious though: although the proposed architecture has become a success for computer
        我们必须谨慎
vision, it is still questionable whether its quality can be attributed to the guiding principles that have

lead to its construction. Making sure would require much more thorough analysis and verification:

for example, if automated tools based on the principles described below would find similar, but

better topology for the vision networks. The most convincing proof would be if an automated

system would create network topologies resulting in similar gains in other domains using the same

algorithm but with very differently looking global architecture. At very least, the initial success of

the Inception architecture yields firm motivation for exciting future work in this direction.

4 Architectural Details
    框架 详情
The main idea of the Inception architecture is based on finding out how an optimal local sparse

structure in a convolutional vision network can be approximated and covered by readily available

dense components. Note that assuming translation invariance means that our network will be built
                        假设翻译不变性
from convolutional building blocks. All we need is to find the optimal local construction and to
                                        
repeat it spatially. Arora et al. [2] suggests a layer-by layer construction in which one should analyze
                        
the correlation statistics of the last layer and cluster them into groups of units with high correlation.

These clusters form the units of the next layer and are connected to the units in the previous layer. 

We assume that each unit from the earlier layer corresponds to some region of the input image and these

units are grouped into filter banks. In the lower layers (the ones close to the input) correlated units

would concentrate in local regions. This means, we would end up with a lot of clusters concentrated

in a single region and they can be covered by a layer of 1×1 convolutions in the next layer, as

suggested in [12]. However, one can also expect that there will be a smaller number of more

spatially spread out clusters that can be covered by convolutions over larger patches, and there

will be a decreasing number of patches over larger and larger regions. In order to avoid patchalignment issues, current incarnations of the Inception architecture are restricted to filter sizes 1×1,
3×3 and 5×5, however this decision was based more on convenience rather than necessity. It also
means that the suggested architecture is a combination of all those layers with their output filter
banks concatenated into a single output vector forming the input of the next stage. Additionally,
since pooling operations have been essential for the success in current state of the art convolutional
networks, it suggests that adding an alternative parallel pooling path in each such stage should have
additional beneficial effect, too (see Figure 2(a)).


    As these “Inception modules” are stacked on top of each other, their output correlation statistics
are bound to vary: as features of higher abstraction are captured by higher layers, their spatial
concentration is expected to decrease suggesting that the ratio of 3×3 and 5×5 convolutions should
increase as we move to higher layers.

    One big problem with the above modules, at least in this na¨ıve form, is that even a modest number of
5×5 convolutions can be prohibitively expensive on top of a convolutional layer with a large number
of filters. This problem becomes even more pronounced once pooling units are added to the mix:
their number of output filters equals to the number of filters in the previous stage. The merging of
the output of the pooling layer with the outputs of convolutional layers would lead to an inevitable

increase in the number of outputs from stage to stage. Even while this architecture might cover the
optimal sparse structure, it would do it very inefficiently, leading to a computational blow up within
a few stages.


    This leads to the second idea of the proposed architecture: judiciously applying dimension reductions and projections wherever the computational requirements would increase too much otherwise.
This is based on the success of embeddings: even low dimensional embeddings might contain a lot
of information about a relatively large image patch. However, embeddings represent information in
a dense, compressed form and compressed information is harder to model. We would like to keep
our representation sparse at most places (as required by the conditions of [2]) and compress the
signals only whenever they have to be aggregated en masse. That is, 1×1 convolutions are used to
compute reductions before the expensive 3×3 and 5×5 convolutions. Besides being used as reductions, they also include the use of rectified linear activation which makes them dual-purpose. The
final result is depicted in Figure 2(b).

    In general, an Inception network is a network consisting of modules of the above type stacked upon
each other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid. For
technical reasons (memory efficiency during training), it seemed beneficial to start using Inception
modules only at higher layers while keeping the lower layers in traditional convolutional fashion.
This is not strictly necessary, simply reflecting some infrastructural inefficiencies in our current
implementation.

    One of the main beneficial aspects of this architecture is that it allows for increasing the number of
units at each stage significantly without an uncontrolled blow-up in computational complexity. The
ubiquitous use of dimension reduction allows for shielding the large number of input filters of the
last stage to the next layer, first reducing their dimension before convolving over them with a large
patch size. Another practically useful aspect of this design is that it aligns with the intuition that
visual information should be processed at various scales and then aggregated so that the next stage
can abstract features from different scales simultaneously.

    The improved use of computational resources allows for increasing both the width of each stage
as well as the number of stages without getting into computational difficulties. Another way to
utilize the inception architecture is to create slightly inferior, but computationally cheaper versions
of it. We have found that all the included the knobs and levers allow for a controlled balancing of
computational resources that can result in networks that are 2 − 3× faster than similarly performing
networks with non-Inception architecture, however this requires careful manual design at this point.

5 GoogLeNet

    We chose GoogLeNet as our team-name in the ILSVRC14 competition. This name is an homage to
Yann LeCuns pioneering LeNet 5 network [10]. We also use GoogLeNet to refer to the particular
incarnation of the Inception architecture used in our submission for the competition. We have also
used a deeper and wider Inception network, the quality of which was slightly inferior, but adding it
to the ensemble seemed to improve the results marginally. We omit the details of that network, since
our experiments have shown that the influence of the exact architectural parameters is relatively

minor. Here, the most successful particular instance (named GoogLeNet) is described in Table 1 for
demonstrational purposes. The exact same topology (trained with different sampling methods) was
used for 6 out of the 7 models in our ensemble.

    The network was designed with computational efficiency and practicality in mind, so that inference
can be run on individual devices including even those with limited computational resources, especially with low-memory footprint. The network is 22 layers deep when counting only layers with
parameters (or 27 layers if we also count pooling). The overall number of layers (independent building blocks) used for the construction of the network is about 100. However this number depends on
the machine learning infrastructure system used. The use of average pooling before the classifier is
based on [12], although our implementation differs in that we use an extra linear layer. This enables
adapting and fine-tuning our networks for other label sets easily, but it is mostly convenience and
we do not expect it to have a major effect. It was found that a move from fully connected layers to
average pooling improved the top-1 accuracy by about 0.6%, however the use of dropout remained
essential even after removing the fully connected layers.

6 Training Methodology

    Our networks were trained using the DistBelief [4] distributed machine learning system using modest amount of model and data-parallelism. Although we used CPU based implementation only, a
rough estimate suggests that the GoogLeNet network could be trained to convergence using few
high-end GPUs within a week, the main limitation being the memory usage. Our training used
asynchronous stochastic gradient descent with 0.9 momentum [17], fixed learning rate schedule (decreasing the learning rate by 4% every 8 epochs). Polyak averaging [13] was used to create the final
model used at inference time.

```

